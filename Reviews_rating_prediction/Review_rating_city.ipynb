{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package & Datasets loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.download('en')\n",
    "# nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Final_Datasets/TA_combined_df_City_tourism_type_VADER_final_Stanza.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Hotel_locID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview & Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER\n",
    "def calculate_compound_score(review):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(review)['compound']\n",
    "\n",
    "# Unreliable tag\n",
    "def calculate_unreliable(row):\n",
    "    compound_score = row['Compound_Score']\n",
    "    rating = row['Review_Rating']\n",
    "    \n",
    "    if (compound_score < -0.49 and rating >= 3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Stanza\n",
    "# def analyze_sentiment(text):\n",
    "#   doc = nlp(text)\n",
    "#   sentiments = [sentence.sentiment for sentence in doc.sentences]\n",
    "  \n",
    "#   compound_score = sentiments[0]\n",
    "#   return compound_score\n",
    "\n",
    "# Stanza x iterations\n",
    "def analyze_sentiment(text):\n",
    "    compound_scores = []\n",
    "    num_iterations=11\n",
    "    for _ in range(num_iterations):\n",
    "        doc = nlp(text)\n",
    "        sentiments = [sentence.sentiment for sentence in doc.sentences]\n",
    "        compound_scores.append(sentiments[0])\n",
    "    \n",
    "    most_common_sentiment = Counter(compound_scores).most_common(1)\n",
    "    # print(compound_scores)\n",
    "    \n",
    "    return most_common_sentiment[0][0]\n",
    "\n",
    "# Stanza Unreliable tag\n",
    "def calculate_unreliable_stanza(row):\n",
    "    stanza_score = row['Stanza_Score']\n",
    "    rating = row['Review_Rating']\n",
    "    \n",
    "    if (stanza_score == 2 and rating < 3) or (stanza_score == 0 and rating >= 3):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Unreliable'] = df.apply(calculate_unreliable, axis=1)\n",
    "print(df['Unreliable'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Stanza_Score'] = df['Review'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unreliable reviews example\n",
    "unreliable_reviews = df[df['Unreliable'] == 1]['Review']\n",
    "unreliable_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews rating distributions\n",
    "rating_counts = df['Review_Rating'].value_counts()\n",
    "rating_counts = rating_counts.sort_index()\n",
    "plt.figure(figsize=(8, 5))  # size\n",
    "plt.bar(rating_counts.index, rating_counts.values)\n",
    "plt.xlabel('Review_Rating')  \n",
    "plt.ylabel('Count')  \n",
    "plt.title('Distribution of Ratings')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracting(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'bout\", \"about\", text)\n",
    "    text = re.sub(r\"\\'til\", \"until\", text)\n",
    "    return text\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "def remove_stopwords(tokens):\n",
    "    texts = [i for i in tokens if i not in stopwords_list]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def lemmatization(tokens):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "  return lemmatized_tokens\n",
    "\n",
    "\n",
    "def word_preprocess(df, column_name):\n",
    "  #lowercase\n",
    "  df[column_name] = df[column_name].apply(lambda x: str(x).lower())\n",
    "\n",
    "  #decontracting\n",
    "  df[column_name] = df[column_name].apply(decontracting)\n",
    "\n",
    "  #remove tags, punctuations, numbers\n",
    "  df[column_name] = df[column_name].apply(lambda x: re.sub('[^a-zA-Z!]', ' ', x))\n",
    "\n",
    "  #tokenization\n",
    "  import nltk\n",
    "  nltk.download('punkt')\n",
    "  df[column_name] = df[column_name].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "  #remove stopwords\n",
    "  df[column_name] = df[column_name].apply(remove_stopwords)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed_df = word_preprocess(df,'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed_df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nan row\n",
    "text_preprocessed_df = text_preprocessed_df.dropna(subset=['Hotel_star', 'Review_Rating', 'Review', 'Reviewer_Contributions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed_df['Hotel_locID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed_df['Review_Rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = text_preprocessed_df['Review_Rating'].value_counts()\n",
    "rating_proportions = text_preprocessed_df['Review_Rating'].value_counts(normalize=True)\n",
    "\n",
    "# 打印計數和比例\n",
    "print(\"Rating counts:\\n\", rating_counts)\n",
    "print(\"Rating proportions:\\n\", rating_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets to sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contributions_range(value):\n",
    "    if value <= 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def hotel_star_range(value):\n",
    "    if value <= 2.0:\n",
    "        return 1\n",
    "    elif value >= 2.5 and value <= 3.0:\n",
    "        return 1\n",
    "    elif value >= 3.5 and value <= 4.0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns\n",
    "text_preprocessed_df['Reviewer_Contributions_range'] = text_preprocessed_df['Reviewer_Contributions'].apply(contributions_range)\n",
    "text_preprocessed_df['Hotel_star_range'] = text_preprocessed_df['Hotel_star'].apply(hotel_star_range)\n",
    "text_preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 4 sub-datasets\n",
    "LCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "LCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 1) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n",
    "HCLS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 1)]\n",
    "HCHS = text_preprocessed_df[(text_preprocessed_df['Reviewer_Contributions_range'] == 2) & (text_preprocessed_df['Hotel_star_range'] == 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split unreliable reviews\n",
    "LCLS_unreliable = LCLS[LCLS['Unreliable'] == 1]\n",
    "LCHS_unreliable = LCHS[LCHS['Unreliable'] == 1]\n",
    "HCLS_unreliable = HCLS[HCLS['Unreliable'] == 1]\n",
    "HCHS_unreliable = HCHS[HCHS['Unreliable'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split reliable reviews\n",
    "LCLS_reliable = LCLS[LCLS['Unreliable'] == 0]\n",
    "LCHS_reliable = LCHS[LCHS['Unreliable'] == 0]\n",
    "HCLS_reliable = HCLS[HCLS['Unreliable'] == 0]\n",
    "HCHS_reliable = HCHS[HCHS['Unreliable'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split subset\n",
    "selected_columns = ['Review', 'Review_Rating']\n",
    "LCLS_text = LCLS_reliable.loc[:, selected_columns]\n",
    "LCHS_text = LCHS_reliable.loc[:, selected_columns]\n",
    "HCLS_text = HCLS_reliable.loc[:, selected_columns]\n",
    "HCHS_text = HCHS_reliable.loc[:, selected_columns]\n",
    "\n",
    "# reset index\n",
    "LCLS_text.reset_index(drop=True, inplace=True)\n",
    "LCHS_text.reset_index(drop=True, inplace=True)\n",
    "HCLS_text.reset_index(drop=True, inplace=True)\n",
    "HCHS_text.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_text['Review'] = [' '.join(text) for text in LCLS_text['Review']]\n",
    "LCHS_text['Review'] = [' '.join(text) for text in LCHS_text['Review']]\n",
    "HCLS_text['Review'] = [' '.join(text) for text in HCLS_text['Review']]\n",
    "HCHS_text['Review'] = [' '.join(text) for text in HCHS_text['Review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sub-datasets to X & Y, Training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X = LCLS_text['Review']\n",
    "LCLS_y = LCLS_text['Review_Rating']\n",
    "\n",
    "LCHS_X = LCHS_text['Review']\n",
    "LCHS_y = LCHS_text['Review_Rating']\n",
    "\n",
    "HCLS_X = HCLS_text['Review']\n",
    "HCLS_y = HCLS_text['Review_Rating']\n",
    "\n",
    "HCHS_X = HCHS_text['Review']\n",
    "HCHS_y = HCHS_text['Review_Rating']\n",
    "\n",
    "LCLS_X_train, LCLS_X_test, LCLS_y_train, LCLS_y_test = train_test_split(LCLS_X, LCLS_y, test_size=0.2, random_state=88)\n",
    "LCHS_X_train, LCHS_X_test, LCHS_y_train, LCHS_y_test = train_test_split(LCHS_X, LCHS_y, test_size=0.2, random_state=88)\n",
    "HCLS_X_train, HCLS_X_test, HCLS_y_train, HCLS_y_test = train_test_split(HCLS_X, HCLS_y, test_size=0.2, random_state=88)\n",
    "HCHS_X_train, HCHS_X_test, HCHS_y_train, HCHS_y_test = train_test_split(HCHS_X, HCHS_y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS \n",
    "LCLS_X_train_bow_df = None\n",
    "LCLS_X_train_tfidf_df = None\n",
    "LCLS_X_train_d2v = None\n",
    "LCLS_X_train_glove = None\n",
    "LCLS_X_train_bert = None\n",
    "LCLS_X_test_bow_df = None\n",
    "LCLS_X_test_tfidf_df = None\n",
    "LCLS_X_test_d2v = None\n",
    "LCLS_X_test_glove = None\n",
    "LCLS_X_test_bert = None\n",
    "\n",
    "# LCHS\n",
    "LCHS_X_train_bow_df = None\n",
    "LCHS_X_train_tfidf_df = None\n",
    "LCHS_X_train_d2v = None\n",
    "LCHS_X_train_glove = None\n",
    "LCHS_X_train_bert = None\n",
    "LCHS_X_test_bow_df = None\n",
    "LCHS_X_test_tfidf_df = None\n",
    "LCHS_X_test_d2v = None\n",
    "LCHS_X_test_glove = None\n",
    "LCHS_X_test_bert = None\n",
    "\n",
    "# HCLS\n",
    "HCLS_X_train_bow_df = None\n",
    "HCLS_X_train_tfidf_df = None\n",
    "HCLS_X_train_d2v = None\n",
    "HCLS_X_train_glove = None\n",
    "HCLS_X_train_bert = None\n",
    "HCLS_X_test_bow_df = None\n",
    "HCLS_X_test_tfidf_df = None\n",
    "HCLS_X_test_d2v = None\n",
    "HCLS_X_test_glove = None\n",
    "HCLS_X_test_bert = None\n",
    "\n",
    "# HCHS\n",
    "HCHS_X_train_bow_df = None\n",
    "HCHS_X_train_tfidf_df = None\n",
    "HCHS_X_train_d2v = None\n",
    "HCHS_X_train_glove = None\n",
    "HCHS_X_train_bert = None\n",
    "HCHS_X_test_bow_df = None\n",
    "HCHS_X_test_tfidf_df = None\n",
    "HCHS_X_test_d2v = None\n",
    "HCHS_X_test_glove = None\n",
    "HCHS_X_test_bert = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def generate_bow_train(X_train, max_features=1000):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    bow_vectors = vectorizer.fit_transform(X_train)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df, vectorizer\n",
    "\n",
    "def generate_bow_test(X_test, vectorizer):\n",
    "    bow_vectors = vectorizer.transform(X_test)\n",
    "    bow_df = pd.DataFrame(bow_vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_bow_df, LCLS_vectorizer = generate_bow_train(LCLS_X_train)\n",
    "LCHS_X_train_bow_df, LCHS_vectorizer = generate_bow_train(LCHS_X_train)\n",
    "HCLS_X_train_bow_df, HCLS_vectorizer = generate_bow_train(HCLS_X_train)\n",
    "HCHS_X_train_bow_df, HCHS_vectorizer = generate_bow_train(HCHS_X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_bow_df = generate_bow_test(LCLS_X_test, LCLS_vectorizer)\n",
    "LCHS_X_test_bow_df = generate_bow_test(LCHS_X_test, LCHS_vectorizer)\n",
    "HCLS_X_test_bow_df = generate_bow_test(HCLS_X_test, HCLS_vectorizer)\n",
    "HCHS_X_test_bow_df = generate_bow_test(HCHS_X_test, HCHS_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def generate_tfidf_train(X_train, stop_words='english', max_features=1000, max_df=0.9):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=max_features, max_df=max_df)\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_train_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df, tfidf_vectorizer\n",
    "\n",
    "def generate_tfidf_test(X_test, tfidf_vectorizer):\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    tfidf_df = pd.DataFrame(X_test_tfidf, columns=feature_names)\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_tfidf_df, tfidf_vectorizer_LCLS = generate_tfidf_train(LCLS_X_train)\n",
    "LCHS_X_train_tfidf_df, tfidf_vectorizer_LCHS = generate_tfidf_train(LCHS_X_train)\n",
    "HCLS_X_train_tfidf_df, tfidf_vectorizer_HCLS = generate_tfidf_train(HCLS_X_train)\n",
    "HCHS_X_train_tfidf_df, tfidf_vectorizer_HCHS = generate_tfidf_train(HCHS_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_tfidf_df = generate_tfidf_test(LCLS_X_test, tfidf_vectorizer_LCLS)\n",
    "LCHS_X_test_tfidf_df = generate_tfidf_test(LCHS_X_test, tfidf_vectorizer_LCHS)\n",
    "HCLS_X_test_tfidf_df = generate_tfidf_test(HCLS_X_test, tfidf_vectorizer_HCLS)\n",
    "HCHS_X_test_tfidf_df = generate_tfidf_test(HCHS_X_test, tfidf_vectorizer_HCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def train_doc2vec_model(X_train, min_count=5, workers=8, epochs=40, vector_size=100):\n",
    "    tagged_docs = [TaggedDocument(doc.split(' '), [i]) for i, doc in enumerate(X_train)]\n",
    "    model = Doc2Vec(min_count=min_count, workers=workers, epochs=epochs, vector_size=vector_size)\n",
    "    model.build_vocab(tagged_docs)\n",
    "    model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different datasets of Doc2vec\n",
    "LCLS_model = train_doc2vec_model(LCLS_X_train)\n",
    "LCHS_model = train_doc2vec_model(LCHS_X_train)\n",
    "HCLS_model = train_doc2vec_model(HCLS_X_train)\n",
    "HCHS_model = train_doc2vec_model(HCHS_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_d2v = np.array([LCLS_model.infer_vector((doc.split(' '))) for doc in LCLS_X_train])\n",
    "LCHS_X_train_d2v = np.array([LCHS_model.infer_vector((doc.split(' '))) for doc in LCHS_X_train])\n",
    "HCLS_X_train_d2v = np.array([HCLS_model.infer_vector((doc.split(' '))) for doc in HCLS_X_train])\n",
    "HCHS_X_train_d2v = np.array([HCHS_model.infer_vector((doc.split(' '))) for doc in HCHS_X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(LCLS_X_train_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(LCHS_X_train_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(HCLS_X_train_d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(HCHS_X_train_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_d2v = np.array([LCLS_model.infer_vector((doc.split(' '))) for doc in LCLS_X_test])\n",
    "LCHS_X_test_d2v = np.array([LCHS_model.infer_vector((doc.split(' '))) for doc in LCHS_X_test])\n",
    "HCLS_X_test_d2v = np.array([HCLS_model.infer_vector((doc.split(' '))) for doc in HCLS_X_test])\n",
    "HCHS_X_test_d2v = np.array([HCHS_model.infer_vector((doc.split(' '))) for doc in HCHS_X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = '../GloVe_wordvec/glove.6B.100d.txt'\n",
    "\n",
    "# import GloVe word vectors into dictionary\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to create embedding matrix\n",
    "def glove_embedding(comment, embeddings_index = embeddings_index, dim=100):\n",
    "    words = comment.split()\n",
    "    vec = np.zeros(dim)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        embedding_vector = embeddings_index.get(word) # get GloVe word vectors\n",
    "        if embedding_vector is not None:\n",
    "            vec += embedding_vector\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "LCLS_X_train_glove = np.array([glove_embedding(comment) for comment in LCLS_X_train])\n",
    "LCHS_X_train_glove = np.array([glove_embedding(comment) for comment in LCHS_X_train])\n",
    "HCLS_X_train_glove = np.array([glove_embedding(comment) for comment in HCLS_X_train])\n",
    "HCHS_X_train_glove = np.array([glove_embedding(comment) for comment in HCHS_X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(LCLS_X_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(LCHS_X_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(HCLS_X_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(HCHS_X_train_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "LCLS_X_test_glove = np.array([glove_embedding(comment) for comment in LCLS_X_test])\n",
    "LCHS_X_test_glove = np.array([glove_embedding(comment) for comment in LCHS_X_test])\n",
    "HCLS_X_test_glove = np.array([glove_embedding(comment) for comment in HCLS_X_test])\n",
    "HCHS_X_test_glove = np.array([glove_embedding(comment) for comment in HCHS_X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT(longformer) model\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = transformers.LongformerTokenizer.from_pretrained(model_name)\n",
    "model = transformers.LongformerModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "def bert_embedding(X_train):\n",
    "    embeddings = []\n",
    "    for text in X_train:\n",
    "        # 將文本轉成BERT的輸入格式，即加上[CLS]與[SEP] token，並轉成tensor\n",
    "        encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "        # 用預訓練BERT模型轉成向量\n",
    "        with torch.no_grad():\n",
    "            model_output = model(encoded_text['input_ids'], attention_mask=encoded_text['attention_mask'])\n",
    "\n",
    "        # 取出[CLS] token對應的向量作為整個文本的向量表示\n",
    "        embeddings.append(model_output.last_hidden_state[:, 0, :].squeeze().tolist())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_bert = bert_embedding(LCLS_X_train)\n",
    "pd.DataFrame(LCLS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCHS_X_train_bert = bert_embedding(LCHS_X_train)\n",
    "pd.DataFrame(LCHS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCLS_X_train_bert = bert_embedding(HCLS_X_train)\n",
    "pd.DataFrame(HCLS_X_train_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCHS_X_train_bert = bert_embedding(HCHS_X_train)\n",
    "pd.DataFrame(HCHS_X_train_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_test_bert = bert_embedding(LCLS_X_test)\n",
    "LCHS_X_test_bert = bert_embedding(LCHS_X_test)\n",
    "HCLS_X_test_bert = bert_embedding(HCLS_X_test)\n",
    "HCHS_X_test_bert = bert_embedding(HCHS_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package loading & function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-30 00:53:37.489589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-30 00:53:37.489614: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-30 00:53:37.492846: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-30 00:53:37.764554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-30 00:53:38.813192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# ML\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# DL\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation function\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "def validation(model_name, X_train, y_train, word_vec_train):\n",
    "    cv = 10\n",
    "    \n",
    "    # MSE\n",
    "    mse_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_squared_error') # mse\n",
    "    mse_scores = -mse_scores # transfer to positive\n",
    "    avg_mse = mse_scores.mean()\n",
    "    \n",
    "    # MAE\n",
    "    mae_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "    mae_scores = -mae_scores  # Convert to positive values\n",
    "    avg_mae = mae_scores.mean()\n",
    "    \n",
    "    # MAPE\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)  # Create custom scorer\n",
    "    mape_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring=mape_scorer)\n",
    "    mape_scores = -mape_scores  # Convert to positive values\n",
    "    avg_mape = mape_scores.mean()\n",
    "    \n",
    "    # R-squared\n",
    "    r2_scores = cross_val_score(model_name, X_train, y_train, cv=cv, scoring='r2')\n",
    "    avg_r2 = r2_scores.mean()\n",
    "\n",
    "    print(f\"{word_vec_train}'s MSE, MAE, MAPE, R^2: {avg_mse}, {avg_mae}, {avg_mape}, {avg_r2}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation\n",
    "def evaluation(y_test, y_pred, word_vec_test):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{word_vec_test}'s MSE, MAE, MAPE, R^2: {mse}, {mae}, {mape}, {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EarlyStopping callback \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 監控驗證集上的損失值\n",
    "    patience=10,  # 如果性能在10個epoch內沒有改善，則停止訓練\n",
    "    verbose=1,  \n",
    "    restore_best_weights=True  # 恢復最佳權重\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize word vector\n",
    "Data = {\n",
    "    'LCLS': {\n",
    "        'tf': LCLS_X_train_bow_df,\n",
    "        'tf-idf': LCLS_X_train_tfidf_df,\n",
    "        'd2v': LCLS_X_train_d2v,\n",
    "        'glove': LCLS_X_train_glove,\n",
    "        'bert': LCLS_X_train_bert,\n",
    "        'tf_test': LCLS_X_test_bow_df,\n",
    "        'tf-idf_test': LCLS_X_test_tfidf_df,\n",
    "        'd2v_test': LCLS_X_test_d2v, \n",
    "        'glove_test': LCLS_X_test_glove, \n",
    "        'bert_test': LCLS_X_test_bert, \n",
    "    },\n",
    "    'LCHS': {\n",
    "        'tf': LCHS_X_train_bow_df,\n",
    "        'tf-idf': LCHS_X_train_tfidf_df,\n",
    "        'd2v': LCHS_X_train_d2v,\n",
    "        'glove': LCHS_X_train_glove,\n",
    "        'bert': LCHS_X_train_bert,\n",
    "        'tf_test': LCHS_X_test_bow_df,\n",
    "        'tf-idf_test': LCHS_X_test_tfidf_df,\n",
    "        'd2v_test': LCHS_X_test_d2v,\n",
    "        'glove_test': LCHS_X_test_glove,\n",
    "        'bert_test': LCHS_X_test_bert,\n",
    "    },\n",
    "    'HCLS': {\n",
    "        'tf': HCLS_X_train_bow_df,\n",
    "        'tf-idf': HCLS_X_train_tfidf_df,\n",
    "        'd2v': HCLS_X_train_d2v,\n",
    "        'glove': HCLS_X_train_glove,\n",
    "        'bert': HCLS_X_train_bert,\n",
    "        'tf_test': HCLS_X_test_bow_df,\n",
    "        'tf-idf_test': HCLS_X_test_tfidf_df,\n",
    "        'd2v_test': HCLS_X_test_d2v,\n",
    "        'glove_test': HCLS_X_test_glove,\n",
    "        'bert_test': HCLS_X_test_bert,\n",
    "    },\n",
    "    'HCHS': {\n",
    "        'tf': HCHS_X_train_bow_df,\n",
    "        'tf-idf': HCHS_X_train_tfidf_df,\n",
    "        'd2v': HCHS_X_train_d2v,\n",
    "        'glove': HCHS_X_train_glove,\n",
    "        'bert': HCHS_X_train_bert,\n",
    "        'tf_test': HCHS_X_test_bow_df,\n",
    "        'tf-idf_test': HCHS_X_test_tfidf_df,\n",
    "        'd2v_test': HCHS_X_test_d2v,\n",
    "        'glove_test': HCHS_X_test_glove,\n",
    "        'bert_test': HCHS_X_test_bert,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model\n",
    "* SVR\n",
    "* Random Forest\n",
    "* XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train = 'tf-idf'\n",
    "word_vec_test = 'tf-idf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_LCLS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_LCHS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_HCLS = SVR(epsilon=0.2, kernel='rbf')\n",
    "svr_HCHS = SVR(epsilon=0.2, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_LCLS.fit(Data['LCLS'][word_vec_train], LCLS_y_train)\n",
    "svr_LCHS.fit(Data['LCHS'][word_vec_train], LCHS_y_train)\n",
    "svr_HCLS.fit(Data['HCLS'][word_vec_train], HCLS_y_train)\n",
    "svr_HCHS.fit(Data['HCHS'][word_vec_train], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_LCLS, Data['LCLS'][word_vec_train], LCLS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_LCHS, Data['LCHS'][word_vec_train], LCHS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_HCLS, Data['HCLS'][word_vec_train], HCLS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(svr_HCHS, Data['HCHS'][word_vec_train], HCHS_y_train, word_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "svr_y_pred_LCLS = svr_LCLS.predict(Data['LCLS'][word_vec_test])\n",
    "svr_y_pred_LCHS = svr_LCHS.predict(Data['LCHS'][word_vec_test])\n",
    "svr_y_pred_HCLS = svr_HCLS.predict(Data['HCLS'][word_vec_test])\n",
    "svr_y_pred_HCHS = svr_HCHS.predict(Data['HCHS'][word_vec_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS\n",
    "evaluation(LCLS_y_test, svr_y_pred_LCLS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCHS\n",
    "evaluation(LCHS_y_test, svr_y_pred_LCHS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCLS\n",
    "evaluation(HCLS_y_test, svr_y_pred_HCLS, word_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCHS\n",
    "evaluation(HCHS_y_test, svr_y_pred_HCHS, word_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train_rf = 'bert'\n",
    "word_vec_test_rf = 'bert_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_LCLS = RandomForestRegressor()\n",
    "rf_LCHS = RandomForestRegressor()\n",
    "rf_HCLS = RandomForestRegressor()\n",
    "rf_HCHS = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_LCLS.fit(Data['LCLS'][word_vec_train_rf], LCLS_y_train)\n",
    "rf_LCHS.fit(Data['LCHS'][word_vec_train_rf], LCHS_y_train)\n",
    "rf_HCLS.fit(Data['HCLS'][word_vec_train_rf], HCLS_y_train)\n",
    "rf_HCHS.fit(Data['HCHS'][word_vec_train_rf], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_LCLS, Data['LCLS'][word_vec_train_rf], LCLS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_LCHS, Data['LCHS'][word_vec_train_rf], LCHS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_HCLS, Data['HCLS'][word_vec_train_rf], HCLS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(rf_HCHS, Data['HCHS'][word_vec_train_rf], HCHS_y_train, word_vec_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "rf_y_pred_LCLS = rf_LCLS.predict(Data['LCLS'][word_vec_test_rf])\n",
    "rf_y_pred_LCHS = rf_LCHS.predict(Data['LCHS'][word_vec_test_rf])\n",
    "rf_y_pred_HCLS = rf_HCLS.predict(Data['HCLS'][word_vec_test_rf])\n",
    "rf_y_pred_HCHS = rf_HCHS.predict(Data['HCHS'][word_vec_test_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "evaluation(LCLS_y_test, rf_y_pred_LCLS, word_vec_test_rf)\n",
    "evaluation(LCHS_y_test, rf_y_pred_LCHS, word_vec_test_rf)\n",
    "evaluation(HCLS_y_test, rf_y_pred_HCLS, word_vec_test_rf)\n",
    "evaluation(HCHS_y_test, rf_y_pred_HCHS, word_vec_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_train_xgb = 'tf'\n",
    "word_vec_test_xgb = 'tf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_LCLS = xgb.XGBRegressor()\n",
    "xgb_LCHS = xgb.XGBRegressor()\n",
    "xgb_HCLS = xgb.XGBRegressor()\n",
    "xgb_HCHS = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_LCLS.fit(Data['LCLS'][word_vec_train_xgb], LCLS_y_train)\n",
    "xgb_LCHS.fit(Data['LCHS'][word_vec_train_xgb], LCHS_y_train)\n",
    "xgb_HCLS.fit(Data['HCLS'][word_vec_train_xgb], HCLS_y_train)\n",
    "xgb_HCHS.fit(Data['HCHS'][word_vec_train_xgb], HCHS_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_LCLS, Data['LCLS'][word_vec_train_xgb], LCLS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_LCHS, Data['LCHS'][word_vec_train_xgb], LCHS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_HCLS, Data['HCLS'][word_vec_train_xgb], HCLS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(xgb_HCHS, Data['HCHS'][word_vec_train_xgb], HCHS_y_train, word_vec_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "xgb_y_pred_LCLS = xgb_LCLS.predict(Data['LCLS'][word_vec_test_xgb])\n",
    "xgb_y_pred_LCHS = xgb_LCHS.predict(Data['LCHS'][word_vec_test_xgb])\n",
    "xgb_y_pred_HCLS = xgb_HCLS.predict(Data['HCLS'][word_vec_test_xgb])\n",
    "xgb_y_pred_HCHS = xgb_HCHS.predict(Data['HCHS'][word_vec_test_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCLS, LCHS, HCLS, HCHS\n",
    "evaluation(LCLS_y_test, xgb_y_pred_LCLS, word_vec_test_xgb)\n",
    "evaluation(LCHS_y_test, xgb_y_pred_LCHS, word_vec_test_xgb)\n",
    "evaluation(HCLS_y_test, xgb_y_pred_HCLS, word_vec_test_xgb)\n",
    "evaluation(HCHS_y_test, xgb_y_pred_HCHS, word_vec_test_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch(Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_X_train_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLS_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: TF-IDF DataFrame and rating(y) transform to PyTorch tensor\n",
    "X = torch.tensor(LCLS_X_train_tfidf_df.values, dtype=torch.float32)\n",
    "y = torch.tensor(LCLS_y_train.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2: contruct Pytorch dataloader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3: define model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.fc3 = nn.Linear(hidden_size, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "input_size = LCLS_X_train_tfidf_df.shape[1]  # 輸入特徵的維度\n",
    "hidden_size = 128  # 隱藏層的神經元數量\n",
    "output_size = 1  # 輸出的評分值\n",
    "\n",
    "model = MLPModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: training\n",
    "criterion = nn.MSELoss()  # 使用均方誤差損失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = 'LCLS'\n",
    "word_vec_train = 'tf'\n",
    "word_vec_test = 'tf_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset][word_vec_train]\n",
    "subdata_y_train = Data[sub_dataset]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset][word_vec_test]\n",
    "subdata_y_test = Data[sub_dataset]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def MLP_model_para(X_train_embedding, params):\n",
    "    nn_model = Sequential()\n",
    "    nn_model.add(Dense(units=int(params['units']), input_dim=X_train_embedding.shape[1], activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(units=int(params['units_h']), activation=params['activation']))\n",
    "    nn_model.add(Dropout(params['dropout']))\n",
    "    nn_model.add(Dense(1, activation=params['activation']))  # Use linear for regression \n",
    "    \n",
    "    nn_model.compile(loss='mean_squared_error', optimizer=params['optimizer'])\n",
    "    return nn_model\n",
    "\n",
    "def objective(params):\n",
    "    model = MLP_model_para(subdata_X_train_embedding.to_numpy(), params)\n",
    "\n",
    "    model.fit(subdata_X_train_embedding.to_numpy(), subdata_y_train.to_numpy(), epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(subdata_X_test_embedding.to_numpy())\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred)\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 32, 256, 32),\n",
    "    'units_h': hp.quniform('units_h', 32, 256, 32),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()  # Create a trials object to track the progress\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "activation = 'relu'\n",
    "batch_size = int(best['batch_size'])\n",
    "dropout = best['dropout']\n",
    "epochs = 70\n",
    "optimizer = 'rmsprop'\n",
    "units = 256\n",
    "units_h = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(X_train_embedding, y_train):\n",
    "  # define model\n",
    "  nn_model = Sequential()\n",
    "  # Input - Layer\n",
    "  nn_model.add(Dense(units=128, input_dim=X_train_embedding.shape[1], activation='relu'))\n",
    "  # Hidden - Layers\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  nn_model.add(Dense(units=64, activation='relu'))\n",
    "  nn_model.add(Dropout(0.3))\n",
    "  # Output- Layer\n",
    "  nn_model.add(Dense(1, activation='relu'))\n",
    "\n",
    "  nn_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = MLP_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores,  mae_scores, mape_scores,  r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "\n",
    "print(f\"{sub_dataset}'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "mlp_model = MLP_model(subdata_X_train_embedding, subdata_y_train)\n",
    "mlp_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "mlp_y_pred = mlp_model.predict(subdata_X_test_embedding)\n",
    "mlp_y_pred = mlp_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, mlp_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset_cnn = 'HCHS'\n",
    "word_vec_train_cnn = 'bert'\n",
    "word_vec_test_cnn = 'bert_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset_cnn][word_vec_train_cnn]\n",
    "subdata_y_train = Data[sub_dataset_cnn]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset_cnn][word_vec_test_cnn]\n",
    "subdata_y_test = Data[sub_dataset_cnn]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# fill in missing values with the mean of the column\n",
    "subdata_X_train_embedding.fillna(subdata_X_train_embedding.mean(), inplace=True)\n",
    "subdata_y_train.fillna(subdata_y_train.mean(), inplace=True)\n",
    "subdata_X_test_embedding.fillna(subdata_X_test_embedding.mean(), inplace=True)\n",
    "subdata_y_test.fillna(subdata_y_test.mean(), inplace=True)\n",
    "\n",
    "def CNN_model_para(X_train_shape, params):\n",
    "    cnn_model = Sequential()\n",
    "    # 1st Conv1D + MaxPooling1D layer  \n",
    "    cnn_model.add(Conv1D(filters=int(params['filters']), kernel_size=int(params['kernel_size']), activation='relu', padding='same', input_shape=(X_train_shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=int(params['filters_1']), kernel_size=int(params['kernel_size']), activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(int(params['pool_size']), padding='same'))\n",
    "    # Flatten\n",
    "    cnn_model.add(Flatten())\n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(params['dropout']))\n",
    "    cnn_model.add(Dense(units=int(params['dense_units']), activation='relu'))\n",
    "    # # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    cnn_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return cnn_model\n",
    "\n",
    "def objective(params):\n",
    "    cnn_model = CNN_model_para(subdata_X_train_embedding.shape, params)\n",
    "    cnn_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred)\n",
    "    # clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'filters': hp.quniform('filters', 32, 256, 32),\n",
    "    'filters_1': hp.quniform('filters_1', 32, 256, 32),\n",
    "    'kernel_size': hp.choice('kernel_size', [7, 9, 11, 13]),\n",
    "    'pool_size': hp.choice('pool_size', [2, 3, 5]),\n",
    "    'dropout': hp.uniform('dropout', 0, 1),\n",
    "    'dense_units': hp.quniform('dense_units', 32, 256, 32),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Can also access the results and losses from th\n",
    "# e trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters: {'batch_size': 32.0, 'dense_units': 192.0, 'dropout': 0.20990214723448342, 'epochs': 2, 'filters': 224.0, 'filters_1': 96.0, 'kernel_size': 3, 'optimizer': 2, 'pool_size': 1}\n",
    "batch_size = int(best['batch_size'])\n",
    "dense_units = int(best['dense_units'])\n",
    "dropout = best['dropout']\n",
    "epochs = 70\n",
    "filters = int(best['filters'])\n",
    "filters_1 = int(best['filters_1'])\n",
    "kernel_size = 9\n",
    "optimizer = 'rmsprop'\n",
    "pool_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(X_train_embedding, y_train):\n",
    "    # define model\n",
    "    cnn_model = Sequential()\n",
    "    \n",
    "    # Conv1D 2 layer + MaxPooling1D layer\n",
    "    cnn_model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same', input_shape=(X_train_embedding.shape[1], 1)))\n",
    "    cnn_model.add(Conv1D(filters=filters_1, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    \n",
    "    # 2nd Conv1D + MaxPooling1D layer\n",
    "    # cnn_model.add(Conv1D(filters=int(filters*2), kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    # cnn_model.add(MaxPooling1D(pool_size, padding='same'))\n",
    "    cnn_model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers\n",
    "    cnn_model.add(Dropout(dropout))\n",
    "    cnn_model.add(Dense(units=dense_units, activation='relu'))\n",
    "    \n",
    "    # Output layer for regression\n",
    "    cnn_model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    cnn_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    y = y.to_numpy()  # Convert Series to NumPy array\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = CNN_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "print(f\"{sub_dataset_cnn}'s MSE, MAE, MAPE, R^2: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "cnn_model = CNN_model(subdata_X_train_embedding, subdata_y_train)\n",
    "cnn_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "cnn_y_pred = cnn_model.predict(subdata_X_test_embedding)\n",
    "cnn_y_pred = cnn_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, cnn_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset_lstm = 'HCHS'\n",
    "word_vec_train_lstm = 'bert'\n",
    "word_vec_test_lstm = 'bert_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "subdata_X_train_embedding = Data[sub_dataset_lstm][word_vec_train_lstm]\n",
    "subdata_y_train = Data[sub_dataset_lstm]['y_train']\n",
    "subdata_X_test_embedding = Data[sub_dataset_lstm][word_vec_test_lstm]\n",
    "subdata_y_test = Data[sub_dataset_lstm]['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape vector to 3D\n",
    "subdata_X_train_embedding = subdata_X_train_embedding.to_numpy().reshape(subdata_X_train_embedding.shape[0], 1, subdata_X_train_embedding.shape[1])\n",
    "subdata_X_test_embedding = subdata_X_test_embedding.to_numpy().reshape(subdata_X_test_embedding.shape[0], 1, subdata_X_test_embedding.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best parameter list\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def lstm_model_para(X_train_embedding, params):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=int(params['units']), return_sequences=params['return_sequences'], input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    lstm_model.add(Dropout(params['dropout']))\n",
    "    lstm_model.add(Dense(units=1, activation='linear'))\n",
    "    lstm_model.compile(optimizer=params['optimizer'], loss='mean_squared_error')\n",
    "    return lstm_model\n",
    "\n",
    "def objective(params):\n",
    "    lstm_model = lstm_model_para(subdata_X_train_embedding, params)\n",
    "\n",
    "    lstm_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=int(params['epochs']), batch_size=int(params['batch_size']), validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "    mse = mean_squared_error(subdata_y_test, y_pred.flatten())\n",
    "    clear_output(wait=True)  # clear output\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'units': hp.quniform('units', 16, 256, 32),\n",
    "    'return_sequences': hp.choice('return_sequences', [True, False]),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'optimizer': hp.choice('optimizer', ['adam', 'rmsprop', 'sgd']),\n",
    "    'epochs': hp.quniform('epochs', 10, 100, 10),\n",
    "    'batch_size': hp.quniform('batch_size', 16, 256, 32)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Also access the results and losses from the trials object\n",
    "losses = [trial['loss'] for trial in trials.results]\n",
    "best_loss = min(losses)\n",
    "print(\"Best MSE:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best['batch_size'])\n",
    "units = int(best['units'])\n",
    "dropout = best['dropout']\n",
    "epochs = 100\n",
    "optimizer = 'adam'\n",
    "return_sequences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train_embedding, y_train): \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=return_sequences, input_shape=(X_train_embedding.shape[1], X_train_embedding.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_metrics(X, y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    # rmse_scores = []\n",
    "    mae_scores = []\n",
    "    mape_scores = []\n",
    "    # corr_scores = []\n",
    "    r2_scores = []\n",
    "\n",
    "    # # X = X.to_numpy()  # Convert DataFrame to NumPy array\n",
    "    # y = y.values.ravel()  # Convert Series to NumPy array\n",
    "    y = y.reset_index(drop=True)  # reset y index\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = LSTM_model(X,y)  # Create a new model for each fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        # rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mse_scores.append(mse)\n",
    "        # rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        mape_scores.append(mape)\n",
    "        # corr_scores.append(corr)\n",
    "        r2_scores.append(r2)\n",
    "\n",
    "    return mse_scores, mae_scores, mape_scores, r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "mse_scores, mae_scores, mape_scores, r2_scores = cross_val_metrics(subdata_X_train_embedding, subdata_y_train)\n",
    "print(f\"{sub_dataset_lstm}'MSE, MAE, MAPE, CC: {np.mean(mse_scores)}, {np.mean(mae_scores)}, {np.mean(mape_scores)}, {np.mean(r2_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "lstm_model = LSTM_model(subdata_X_train_embedding, subdata_y_train)\n",
    "lstm_model.fit(subdata_X_train_embedding, subdata_y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "lstm_y_pred = lstm_model.predict(subdata_X_test_embedding)\n",
    "lstm_y_pred = lstm_y_pred.ravel() # covert to 1-dim\n",
    "\n",
    "# model evaluation\n",
    "evaluation(subdata_y_test, lstm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LSTM model and architecture to single file\n",
    "lstm_model.save(\"Model/HCHS_lstm_model_longformer.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
